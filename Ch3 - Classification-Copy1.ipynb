{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Classification#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Classification Template ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification template\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "# Create your classifier here\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Classifier (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualising the Test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Classifier (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You try to predict the probability that a user converts given his age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula: $ln(\\frac{p}{1-p}) = b_0 + b_1 * x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-NN ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the k Neighbors, how many are category 1? How many are category 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM & Kernel SVM ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of that method is to seperate the two classes through an hyperplane. This hyperplane should maximize the distance between the two closest instances of the classes. That hyperplane is the maximum margin classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual algorithms use the paragon of each class. The most appley apple and orangey orange. And then it will compare the new instances and compute how similar they are to each 'idea' of the class.\n",
    "SVMs work the other way round: the most orangey apple and the most appley orange. What's different between them?\n",
    "\n",
    "Those are called the support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: There are other functions for the kernel than simply linear which doesn't do much more than a linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: Gaussian kernel for a circle decision boundary. Sigmoid kernel, Polynomial kernel, Laplacian, etc...\n",
    "\n",
    "Some visualisation are available at https://mlkernels.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem: $ P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|not A)P(not A)}$\n",
    "\n",
    "or less general $ P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have: $ Posterior   Probability = \\frac{Likelihood * Prior   Probability}{Marginal   Likelihood} $\n",
    "\n",
    "With: \n",
    "- Posterior Probability = Probability that a sample is of category A given that it is in the similar area\n",
    "- Likelihood = Probability that a sample in the area deemed similar is of category A\n",
    "- Prior Probability = Probability of A over the total population\n",
    "- Marginal Likelihood = Probability that a sample is in the area deemed similar to the new datapoint\n",
    "    \n",
    "What's cool is that we get to fidget with the diameter of the similar area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: \n",
    "1. It is called Na√Øve because it assumes independance, and it still gives good results when variable are not independent\n",
    "2. When comparing P(A|B) and P(notA|B) sometimes people omit the Marginal Likelihood since it is the same on both side: P(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Tree Classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite similar to the Decision Tree Regression. It had kind of died off but came back with new methods such as Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe not use the feature scaling: not necessary. Especially for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Forest Classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical ensemble learning.\n",
    "1. Pick at random K data points from the original set\n",
    "2. Build the decision tree associated with these K data points\n",
    "3. Choose N the number of trees I want to build, loop over 1 and 2\n",
    "4. Pick a function (e.g. average or median) to apply to the N decision trees for each new points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating a model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **False positives (type 1 error) should have been negative and False negatives (type 2 error) should have been positive**\n",
    "+ **Confusion Matrix**\n",
    "\n",
    "|||      **y^ predicted value**         |\n",
    "| -------------- | -------------- | -------------- | -------------- |\n",
    "|                        |    |         0      |      1         |\n",
    "|      **y actual value**| 0  | True Negative  | False Positive |\n",
    "|      **y actual value**| 1  | False Negative | True Positive  |\n",
    "\n",
    "    Accuracy ratio = Correct / Total\n",
    "    Error ratio = Error / Total\n",
    "\n",
    "+ **Accuracy paradox**\n",
    "    \n",
    "    Here are two confusion matrices, the second has no model but still has a better accuracy\n",
    "    \n",
    "|||      **y^ predicted value**         |\n",
    "| -------------- | -------------- | -------------- | -------------- |\n",
    "|                        |    |         0      |      1         |\n",
    "|      **y actual value**| 0  | 9700  | 150 |\n",
    "|      **y actual value**| 1  | 50 | 100  |\n",
    "\n",
    "    Accuracy ratio = 9800 / 10000 = 98%\n",
    "\n",
    "|||      **y^ predicted value**         |\n",
    "| -------------- | -------------- | -------------- | -------------- |\n",
    "|                        |    |         0      |      1         |\n",
    "|      **y actual value**| 0  | 9850  | 0 |\n",
    "|      **y actual value**| 1  | 150 | 0  |    \n",
    "\n",
    "    Accuracy ratio = 9850 / 10000 = 98.5%\n",
    "\n",
    "+ **Cumulative Accuracy Profile**\n",
    "    \n",
    "    Rank the customers to contact first to augment the surface above the diagonal line of the random sample. A CAP curve is not a ROC curve although they look very similar!! ROC = Receiver Operating Characteristics.\n",
    "    \n",
    "    **CAP:** Total Purchased = f(Total Contacted) <br>\n",
    "    **ROC:** True Positive Rate = f(False positive Rate)\n",
    "    \n",
    "+ How to assess a CAP curve\n",
    "\n",
    "    Calculate the area under the curve and above random then divide it by the perfect model. <br>\n",
    "    Or... Take the vertical 50% and plot it on the CAP curve.\n",
    "    \n",
    "| Value at 50%   |   Quality      |\n",
    "| -------------- | -------------- |  \n",
    "| X < 60%        |   Rubbish      | \n",
    "| 60% < X < 70%  |   Poor         | \n",
    "| 70% < X < 80%  |   Good         | \n",
    "| 80% < X < 90%  |   Very Good    | \n",
    "| 90% < X < 100% |   Suspect      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
