{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Dimensionality Reduction #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular Dimensionality Reduction technique are called:\n",
    "- Principal Component Analysis\n",
    "- Linear Discriminant Analysis\n",
    "- Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cases are:\n",
    "- Noise reduction\n",
    "- Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principal Component Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to identify and detect patterns for the data and to find correlation between variables. This is useful to create a new version of the data with less dimensions but same information. Way easier and faster to analyze. It does not attempt to predict the values, it focuses on finding a relation between them. It is however highly affected by outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process is to apply PCA **after scaling** the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step to perform first and then delete or comment out\n",
    "# We first want to see how much the variance can be explained by a certain number of variables\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = None)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first step helps us choosing how many principal components we keep and the cumulative explained variance. Once we've done that, we can just modify our code and put the right number of n_components. Here we pick 2 so that we can then plot it on a 2d chart and because 56% of the variance is explained which is an OK number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Discriminant Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has overall the same goal: reduce the dimensionality of our dataset. But it also focuses on something different: maximizing the difference between classes. While PCA is unsupervised, LDA is supervised because of the relation to the dependant variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this one we don't use any 'None' like before because we want to be able to plot it on a 2d graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 2)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernel  Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for nonlinear problems. We use a kernel trick and a Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components = 2, kernel = 'rbf')\n",
    "X_train = kpca.fit_transform(X_train)\n",
    "X_test = kpca.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
