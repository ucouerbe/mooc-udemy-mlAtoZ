{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Deep Learning #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models can be used for:\n",
    "- Artificial Neural Networks for Regression and Classification\n",
    "- Convolutional Neural Networks for Computer Vision\n",
    "- Recurrent Neural Networks for Time Series Analysis\n",
    "- Self Organizing Maps for Feature Extraction\n",
    "- Deep Boltzmann Machines for Recommendation Systems\n",
    "- Auto Encoders for Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept is to mimic how a brain learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Artificial Neural Networks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics of interest:\n",
    "- **Neuron**\n",
    "    <br>Dendrite and axon, it takes input, does a simple operation, applies an activation function and gives an output.\n",
    "    <br>*Interesting reading about Neurons can be Efficient Backprop by LeCun 1998*\n",
    "- **Activation function**\n",
    "    <br>Different types are threshold (0 if <0 else 1), sigmoid (1 / 1 + $e^{-x}$), rectifier (max(0,x)), hyperbolic tangent (1 - $e^{-2x}$ / 1 + $e^{-2x}$).\n",
    "    <br>*Interesting reading about Activation functions can be Deep Sparse Rectifier neural networks by Glorot 2011*\n",
    "- **How do Neural Networks work**\n",
    "    <br>They do very simple operations with all the previous input and combining them, they produce an output.\n",
    "- **How do they learn**\n",
    "    <br>Thanks to the cost function, they compare $\\hat{y}$ and $y$ and then reajust the weights. An epoch is one round of training with the full dataset.\n",
    "    <br> *Interesting list of cost functions can be found in CrossValidated (2015)*\n",
    "- **Gradient descent**\n",
    "    <br>How the weights are adjusted?\n",
    "    <br>First instinct would be brute force, but it is way too computationally intensive especially given the curse of dimensionality: each input has a weight in each node. The number of operations rapidly becomes impossible.\n",
    "    <br>Hence the gradient descent method with classic derivation methods. Problem is, the cost function needs to be convex and even if it is then it can still very well converge towards a local minimum.\n",
    "- **Stochastic Gradient descent**\n",
    "    <br>And this is where enters stochastic gradient descent which doesn't require for the cost function to be convex! Contrary to the previous batch gradient descent, you adjust the weights after each row and not after a total batch. Although batch gradient descent is a deterministic method, stochastic GD is more likely to find the global minimum. But it is a stochastic method i.e. partly random, so it will fluctuate more. It won't take longer since it doesn't need to load the full dataset weights into memory but it will fluctuate more.\n",
    "    <br> *Interesting reading can be A Neural Network in 13 lines of Python by Trask (Part 2 - Gradient Descent) (2015)*\n",
    "    <br> *Something else a bit harder, Neural Networks and Deep Learning by Michael Nielsen (chap 2) (2015)*\n",
    "- **Backpropagation**\n",
    "    <br>The key strength is that it adjusts all the weights at the same time.\n",
    "    <br> *Something to read, Neural Networks and Deep Learning by Michael Nielsen (chap 2) (2015)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business problem example:\n",
    "- Churn prediction on a bank sample with different characteristics + a variable named 'exited' if they exited the bank in the following 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available librairies are Theano, Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process is:\n",
    "+ **Step 1**: Classic classification problem\n",
    "+ **Step 2**: Take care of eventual categorical data\n",
    "+ **Step 3**: Apply feature scaling to help the machine and ease the number of calculations\n",
    "+ **Step 4**: Import Keras and packages\n",
    "+ **Step 5**: Initialise the ANN\n",
    "+ **Step 6**: Create the layers\n",
    "+ **Step 7**: Compile\n",
    "+ **Step 8**: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ulysse/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**: How many nodes in each hidden layer?\n",
    "- take the average of the number of nodes in the input layer and the output layer\n",
    "\n",
    "**NB2**: What's the equivalent of sigmoid for classification problem with more than 2 categories?\n",
    "- it's called softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Networks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics of interest:\n",
    "- **What is a CNN**\n",
    "    <br>Classifies images for example. You can treat a B&W image through each 0-255 value of each pixel. Even a colored image through 3 different layers of RBG of 0-255 values. For example with a face, it can classify its emotion.\n",
    "    <br>*Interesting reading is Gradient Based Learning applied to Document Recognition by Yann LeCun 1998*\n",
    "- **Convolution operation**\n",
    "    <br>You apply a feature detector (like a 3x3 matrix of booleans) to a bigger image (like a 14x14 image). This gives a feature map of values of where the feature detector caught positive signals. The important part is that the feature map is smaller than the first image. We are loosing information but we detect features which is what matters. Plus we create a lot of different feature maps to obtain our first convolution layer.\n",
    "    <br>*Interesting reading is Introduction to Convolutional Neural Networks by Jianxin Wu 2017 see http://cs.nju.edu.cn/wujx/*\n",
    "- **ReLU**\n",
    "    <br>This function helps breaking up linearity and removes all the negative part to focus on the positive signals.\n",
    "    <br>*Interesting reading is Understanding Convolutional Neural Network with a Mathematical model by C C Jay Kuo 2016*\n",
    "    <br>*Interesting reading is Delving deep into rectifiers: Surpassing Human Level Performance by Kaiming He 2015*\n",
    "- **Pooling**\n",
    "    <br>When you try to recognize a pattern or a specific thing, the model has to have spatial invariance i.e. it should not care as to where in the image the feature might appear but most importantly if the feature is a bit tilted or warped in some way. This is where different pooling come in. Max pooling for example, you apply a max matrix to each subpart of the feature map. You keep the important information through the max and reduce the size again. We obtain a pooled feature map. It helps accounting for any possible spatial distortion. It also prevent overfitting because it doesn't keep extra information.\n",
    "    <br>*Interesting reading and fairly easy to read is Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition by Dominik Scherer 2010*\n",
    "    <br> *Interesting link: http://scs.ryerson.ca/~aharley/vis/conv/flat.html*\n",
    "- **Flattening**\n",
    "    <br>It's basically just put everything into a column of input for the input layer of the ANN.\n",
    "- **Full Connection**\n",
    "    <br>An hidden layer in the new ANN is called a fully connected layer as each node is connected to all the previous and next nodes. The role of those layers is to combine our features into more elaborate features and into attributes that are good predictors of a class. What's different than a previous ANN is that we have multiple output nodes at the end. You need one output for each class. After one forwrd prediction an error is predicted and then it's backpropagated through the network.\n",
    "- **Summary**\n",
    "    <br>*Interesting reading is The 9 deep learning papers you need to know about by Deshpande in 2016*\n",
    "- **Softmax and Cross Entropy**\n",
    "    <br>In the n output nodes at the end, the value don't have to add up to one. They do because we apply a function called Softmax. As for Cross entropy it's a cost function to rank the output of a NN. You have Mean-Squared Errors and Cross Entropy for example. CE > MSE if for example the values are really small, the gradient descent will take a lot more time with the MSE.\n",
    "    <br>*Interesting video called the softmax function*\n",
    "    <br>*Interesting reading is A Friendly introduction to Cross Entropy loss by DiPietro 2016*\n",
    "    <br>*Interesting reading is How to implement a NN intermezzo 2 by Roelants 2016*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution step**: pick the number of feature maps we are looking for (here 32) and correctly input the input shape (size and # of channels) as well as the activation function. This step is important because we also extract the special information. Not only on each pixel but also on what's around each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling step**: We keep the information but reduce by 2 the size of the feature maps because if we didn't do that we would have too many input nodes after the Flattening step and the model would run for ages. Using a 2 by 2 matrix is the norm for a max pooling. There are other kinds of pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Convolution step**: The first time we ran our model with only one convolution step we had a significant difference of the accuracy of the training and the test set. This means that we had overfitting. A good way to prevent that and to make our model run faster is to have a second convolution step. Here we don't need to specify the input shape as the model already knows what it's producing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flattening step**: This step consists in transforming all the above information contained in feature maps into a single input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full connection step**: In this step we create a classic ANN with the input vector we designed in the above steps. Be careful of the output dimensions and pick well the activation functions. The first output dimension, a power of 2 is a best practice. Relu here is the classic hidden layer activation function and Sigmoid is to give a probability output, if we had more than 2 categories we would need Softmax. Here it is 1 because we only have 2 categories, were it not the case we would need 1 dimension for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling step**: We need to pick the optimize, the loss function and a list of the metrics to calculate the efficiency of our model. For more than two categories we use 'categorical_crossentropy' as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 8000,\n",
    "                         nb_epoch = 25,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the images step**: We do this to prevent overfitting. As we do not have a lot of images, the first function we use is to create more examples of the training set. We apply tiny changes to the images and therefore augment the training set. A lot of examples of good functions such as this one can be found in the keras documentation. What's important here is to pick the right target sizes, small enough batch sizes and nb of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end if you want to have a better accuracy, you can add more convolution layers and increase the target size to extract more information out of our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
